% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{amssymb}
% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Applications of PCA Denoising in Image Classification}
\author{Andrew Duenner \\ Alex Kimn}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section*{Preferred Presentation Date}
May 17, 2018
\section*{Programming Libraries}
\begin{itemize}
  \item Language: Python 3
  \item Libraries: OpenCV, Numpy, pyTorch, PyBM3D
\end{itemize}

\section*{Algorithm}
We will implement principal component analysis as a method to reduce the dimension of an image represented as a matrix. Our primary goal is to reduce the noise in an image represented by a matrix $A \in \mathbb{R}^{m,m} $. PCA allows us to pick the subspace  with dimension $k\ll m$ that it most representative of the variance  in the image matrix. 
Two promising methods for efficiently calculating PCA for very small $k$ are the NIPALS method \cite{GELADI19861} and power iteration.  For calculating many principal components, these methods can be susceptible to loss of orthogonality and an alternative method must be used. The most common method for calculating a large number of principal components involves calculating singular values \cite{trefethen1997numerical} \cite{cline2006computation}. 
Calculating singular values with high accuracy requires two steps: bidiagonalization and computing the SVD of the bidiagonal matrix \cite{trefethen1997numerical}. For bidiagonalization we will implement Golub-Kahan Bidiagonalization \cite{trefethen1997numerical} and for computation of the singular values we will explore divide and conquer methods as well as methods using givens rotations \cite{cline2006computation}. In our analysis we will comment on which of our implementations are most favorable to cheaply calculating a sufficient number of principal components to denoise an image. Once the PCA transform is calculated, the inverse transformation is applied to obtain the final denoised image.

\section*{Competing Algorithms}
\begin{itemize}
  \item \textbf{Control:} Apply no denoising filter
  \item \textbf{LPG-PCA} \cite{zhang2010two}:   Two-stage PCA with local pixel grouping prior to the second stage
  \item \textbf{Block Matching} \cite{dabov2006image}: Extracts similar image fragments into a 3D-cylinders, each of which is run through a filter and then aggregated back to the original 2D-shape
\item \textbf{Auto-Encoder} \cite{vincent2010stacked}:  Train a robust representation from the corrupted images using encoder/decoder pair; use encoder output to classify \textit{time permitting}
\end{itemize}

\section*{Benchmark Problems}
\begin{itemize}
\item \textbf{MNIST} 70,000 28x28 black and white images of digits; 10-way classification
\item \textbf{CIFAR-10} 32x32 color images of everyday objects; 10-way classification problem
\item \textbf{UMD Faces} $>$ 300,000 relatively large images of human faces; $>$ 8,000-way classification problem; we will likely use a reduced version with only the most represented people
\end{itemize}

\section*{Evaluation}

 We will evaluate the output of these various algorithms in two ways. Firstly, we will compare the denoised image to the original and use the norms of the residual as a metric to evaluate the algorithms' performance. 
However, since the original images themselves may be noisy, in order to evaluate the quality of the denoising with respect to the end application (classification), we will utilize each of the algorithms as a pre-processing filter for a CNN (Convolutional Neural Network) trained on each of the benchmark problems. 
Here, we will be use the accuracy increase of the classifier to evaluate the effectiveness of the denoising algorithms. This will be done across different severities and types (Gaussian, contrast, etc.) of noise to see how robust of a classifier can be created with each algorithm.


\nocite{*}

\bibliography{bibliography}
\bibliographystyle{plain}{}
\end{document}
